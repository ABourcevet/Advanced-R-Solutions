---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r, include=FALSE}
source("common.R")
```

# Improving performance

In this chapter we will run a few benchamrks and use the following helper-function to subset and pretty-print their results.

```{r, include=FALSE}
library(rlang)

print_bench_results <- function(bench_df, ...) {
  dots <- rlang::enquos(...)
  
  bench_df %>% 
    purrr::modify_at("expression", as.character) %>% 
    dplyr::select(!!!dots)
}
```


## Checking for existing solutions

1. __[Q]{.Q}__: What are faster alternatives to `lm`? Which are specifically designed to work with larger datasets?

   __[A]{.solved}__: The [Cran Task View for High Performance Computing](https://cran.rstudio.com/web/views/HighPerformanceComputing.html) provides many recommendations. For this question, we are most interested in the section on "Large memory and out-of-memory data". We could for example give `biglm::biglm()`, `speedglm::speedlm()` or `RcppEigen::fastLm()` a try.
   
   For small datasets, there will be only minor performance gains (or even a small cost):

    ```{r}
    bench::mark(
      "lm" = lm(
        body_mass_g ~ bill_length_mm + species, data = penguins
        ) %>% coef(),
      "biglm" = biglm::biglm(
        body_mass_g ~ bill_length_mm + species, data = penguins
        ) %>% coef(),
      "speedglm" = speedglm::speedlm(
        body_mass_g ~ bill_length_mm + species, data = penguins
        ) %>% coef(),
      "fastLm" = RcppEigen::fastLm(
        body_mass_g ~ bill_length_mm + species, data = penguins
      ) %>% coef()
    )
    ```
    
   However for bigger data it will make a difference:
    
    ```{r,collapse = TRUE, warning=FALSE}
    eps <- rnorm(100000)
    x1 <- rnorm(100000, 5, 3)
    x2 <- rep(c("a", "b"), 50000)
    y <- 7 * x1 + (x2 == "a") + eps
    td <- data.frame(y = y, x1 = x1, x2 = x2, eps = eps)

    bench::mark(
      "lm" = lm(y ~ x1 + x2, data = td) %>% coef(),
      "biglm" = biglm::biglm(y ~ x1 + x2, data = td) %>% coef(),
      "speedglm" = speedglm::speedlm(y ~ x1 + x2, data = td) %>% coef(),
      "fastLm" = RcppEigen::fastLm(y ~ x1 + x2, data = td) %>% coef()
    )
    ```
    
   For further speed improvements, you could install a linear algebra library optimized for your system (see `?speedglm::speedlm`).
    
   > The functions of class 'speedlm' may speed up the fitting of LMs to large data sets. High performances can be obtained especially if R is linked against an optimized BLAS, such as ATLAS.
    
2. __[Q]{.Q}__: What package implements a version of `match()` that's faster for repeated lookups? How much faster is it?
    
   __[A]{.solved}__: A web search points us to the `{fastmatch}`-package. We compare it to `base::match()` and observe an impressive relative performance gain (up to 35x faster computation).
    
    ```{r}
    set.seed(1)
    table <- 1L:100000L
    x <- sample(table, 10000, replace = TRUE)
    
    bench::mark(
      "match" = match(x, table),
      "fastmatch" = fastmatch::fmatch(x, table),
      relative = TRUE
    ) 
    ```
    
3. __[Q]{.Q}__: List four functions (not just those in base R) that convert a string into a date time object. What are their strengths and weaknesses?
    
   __[A]{.solved}__: The usual base R way is to use the `as.POSIXct()` generic and create a date time object of class `POSIXct` and type `integer`.
   
    ```{r}
    date_ct <- as.POSIXct("2020-01-01 12:30:25")
    date_ct
    ```
   
   Under the hood `as.POSIXct()` employs `as.POSIXlt()` for the character conversion. This creates a date time object of class `POSIXlt` and type `list`.
   
    ```{r}
    date_lt <- as.POSIXlt("2020-01-01 12:30:25")
    date_lt
    ```

   The `POSIXlt` class has the advantage that it carries the individual time components as attributes. This allows to extract the time components via typical list operators.
    
    ```{r}
    attributes(date_lt)
    date_lt$sec
    ```
   
   However, while `lists` may be practical basic calculations are often faster and require less memory for objects with underlying `integer` type.
   
    ```{r}
    date_lt2 <- rep(date_lt, 10000)
    date_ct2 <- rep(date_ct, 10000)
    
    bench::mark(
      date_lt2 - date_lt2, 
      date_ct2 - date_ct2
    )
    ```

   Although both date time classes inherit from POSIXt, many functions only dispatch on the first argument and don't account for the possibility of mixed date time object usage. Therefore, better decide for an appropriate date time class, instead of mixing them.
   
    ```{r}
    c(date_lt, date_ct)
    c(date_ct, date_lt)
    ```
    
   `as.POSIXlt()` in turn uses `strptime()` under the hood, which creates a similar date time object.
   
    ```{r}
    date_str <- strptime("2020-01-01 12:30:25",
                         format = "%Y-%m-%d %H:%M:%S")
    identical(date_lt, date_str)
    ```

   `as.POSIXct()` and `as.POSIXlt()` accept different character inputs by default (e.g. "2001-01-01 12:30", "2001/1/1 12:30"). `strptime()` requires the format argument to be set explicitly, but also offers an increase in performance.

    ```{r}
    bench::mark(
      as.POSIXct = as.POSIXct("2020-01-01 12:30:25"),
      as.POSIXct_format = as.POSIXct("2020-01-01 12:30:25",
                                     format = "%Y-%m-%d %H:%M:%S"),
      strptime_fomat = strptime("2020-01-01 12:30:25",
                                format = "%Y-%m-%d %H:%M:%S")
    ) %>% 
      print_bench_results(expression, median)
    ```
    
   A fourth way is to use the converter functions from the `{lubridate}` package, which contains wrapper functions (for the POSIXct approach) with an intuitive syntax. (There is a slight decrease in performance though.)

    ```{r}
    library(lubridate)
    ymd_hms("2013-07-24 23:55:26")
    
    bench::mark(
      as.POSIXct = as.POSIXct("2013-07-24 23:55:26", tz = "UTC"),
      ymd_hms = ymd_hms("2013-07-24 23:55:26")
    ) %>% 
      print_bench_results(expression, median)
    ```
    
   For additional ways to convert characters into date time objects, have a look at the `{chron}`, the `{anytime}` and the `{fasttime}` packages. The `{chron}` package introduces new classes and stores times as fractions of days in the underlying double type, while it doesn't deal with timezones and daylight savings. The `{anytime}` package aims to convert "Anything to POSIXct or Date". The `{fasttime}` package contains only one function (`fastPOSIXct()`).

4. __[Q]{.Q}__: Which packages provide the ability to compute a rolling mean?
    
   __[A]{.solved}__: To compute a rolling mean the following packages and functions can be used:
   
    ```{r}
    zoo::rollmean(1:10, 2, na.pad = TRUE, align = "left")
    zoo::rollapply(1:10, 2, mean, fill = NA, align = "left")

    TTR::SMA(1:10, 2)

    RcppRoll::roll_mean(1:10, n = 2, fill = NA, align = "left")
    
    caTools::runmean(1:10, k = 2, endrule = "NA", align = "left")
    ```

   Note that an exhaustive example on how to create a rolling mean function is provided in the [textbook](http://adv-r.had.co.nz/Functionals.html).

5. __[Q]{.Q}__: What are the alternatives to `optim()`?
    
   __[A]{.solved}__: The optimal choice depends on the use case: for a general overview we would suggest the corresponding [Taskview on Optimization](https://cran.r-project.org/web/views/Optimization.html).


## Doing as little as possible

1. __[Q]{.Q}__: What's the difference between `rowSums()` and `.rowSums()`?
    
   __[A]{.solved}__: When we inspect the source code of the user-facing `rowSums()`, we see that it is designed as a wrapper around `.rowSums()` with some input validation, conversions and handling of a special case (complex numbers).
    
    ```{r}
    rowSums
    ```

  `.rowSums()` calls an internal function, which is build into the R interpreter. These compiled functions can be very fast.
    
    ```{r}
    .rowSums
    ```
    
   If the convenient features of `rowSums()` are not needed, then using `.rowSums()` will be more performant.

    ```{r}
    m <- matrix(rnorm(1e8), nrow = 10000)
    
    bench::mark(rowSums(m),
                .rowSums(m, 10000, 10000))
    ```

2. __[Q]{.Q}__: Make a faster version of `chisq.test()` that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying `chisq.test()` or by coding from the [mathematical definition](http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).
    
   __[A]{.solved}__: We aim to speed up our reimplementation of `chisq.test()` by *doing less*.
    
    ```{r}
    chisq.test2 <- function(x, y){
      
      # Input Validation
      if (!is.numeric(x)) {
        stop("x must be numeric")}
      if (!is.numeric(y)) {
        stop("y must be numeric")}
      if (length(x) != length(y)) {
        stop("x and y must have the same length")}
      if (length(x) <= 1) {
        stop("length of x must be greater one")}
      if (any(c(x, y) < 0)) {
        stop("all entries of x and y must be greater or equal zero")}
      if (sum(complete.cases(x, y)) != length(x)) {
        stop("there must be no missing values in x and y")}
      if (any(is.null(c(x, y)))) {
        stop("entries of x and y must not be NULL")}
      
      # Help variables
      m <- rbind(x, y)
      margin1 <- rowSums(m)
      margin2 <- colSums(m)
      n <- sum(m)
      me <- tcrossprod(margin1, margin2) / n
      
      # Output
      x_stat = sum((m - me)^2 / me)
      dof <- (length(margin1) - 1) * (length(margin2) - 1)
      p <- pchisq(x_stat, df = dof, lower.tail = FALSE)
      
      list(x_stat = x_stat, df = dof, `p-value` = p)
    }
    ```
    
   We need to check if our new implementation returns the same results.
    
    ```{r}
    a <- 21:25
    b <- seq(21, 29, 2)
    m_test <- cbind(a, b)
    
    chisq.test(m_test)
    chisq.test2(a, b)
    ```
   
   Finally we benchmark this implementation and compare it with the original `stats::chisq.test()` as well as a compiled version of itself.
    
    ```{r}
    chisq.test2c <- compiler::cmpfun(chisq.test2)
    
    bench::mark(
      chisq.test(m_test),
      chisq.test2(a, b),
      chisq.test2c(a, b),
      check = FALSE
    ) %>%
      print_bench_results(expression, median)
    ```

3. __[Q]{.Q}__: Can you make a faster version of `table()` for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?
    
   __[A]{.solved}__: When analysing the source code of `table()`, we try to omit everything unnecessary and extract the main building blocks. That's where we realize that `table()` is mainly powered by `tabulate()`, which is a very fast counting function. However, we can not simply apply `tabulate()` to our two integer vectors directly. Instead, we need to preprocess them accordingly, to match `tabulate()`'s expected input format. Therefore, the integer pairs in our two vectors need to be mapped to their corresponding index in the final output table (which is ordered columnwise in this implementation). The basic steps to get to this mapping can be adopted from `table()`'s source code again. So the main challenge is to compute the preprocessing steps as performant as possible.

   First, we calculate the dimensions and names of the output table. Here we need to apply `unique()`, `sort()` and `length()` to our integer vectors. Next, we map the elements of each vector according to their position within the vector itself (i.e. the smallest value is mapped to `1L`, the second smallest value to `2L`, etc.). One fast way to do these lookups is to employ the combination of `match(x, sort(unique(x)))` or (even better) `fastmatch::fmatch(x, sort(unique(x)))`. Following the logic within `table()`, we combine and shift these looked up values to create a mapping of integer pairs in our data to the index of the output table. After applying the lookup, `tabulate()` counts the values for us and is returning an integer vector which contains counts for each position in our the table. As a last step, we reuse the code from `table()` to assign the correct dimension and class.

    ```{r}
    table2 <- function(a, b){
    
      s_u_a <- sort(unique(a))
      s_u_b <- sort(unique(b))
  
      l_u_a <- length(s_u_a)
      l_u_b <- length(s_u_b)
  
      dims <- c(l_u_a, l_u_b)
      pr <- l_u_a * l_u_b
      dn <- list(a = s_u_a, 
                 b = s_u_b)

      bin <- fastmatch::fmatch(a, s_u_a) +
        l_u_a * fastmatch::fmatch(b, s_u_b) - l_u_a
      y <- tabulate(bin, pr)
      
      y <- array(y, dim = dims, dimnames = dn)
      class(y) <- "table"
      
      y
    }
       
    a <- sample(100, 10000, TRUE)
    b <- sample(100, 10000, TRUE)
    
    bench::mark(table(a, b),
                table2(a, b),
                relative = TRUE)
    ```

   <!-- TODO: we didn't apply this to our chi-square function. Is this okay? :) -->

## Vectorise

1. __[Q]{.Q}__: The density functions, e.g., `dnorm()`, have a common interface. Which arguments are vectorised over? What does `rnorm(10, mean = 10:1)` do?
    
   __[A]{.solved}__: We can get an overview of the interface of these functions via `?dnorm`:
    
    ```{r, eval = FALSE}
    dnorm(x, mean = 0, sd = 1, log = FALSE)
    pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
    qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
    rnorm(n, mean = 0, sd = 1)
    ```
    
   These functions are vectorised over their numeric arguments, which includes the first argument (`x`, `q`, `p`, `n`) as well as `mean` and `sd`. Note that it's dangerous to supply a vector to `n` in the `rnorm()` function, because the behaviour will change, when `n` has length 1.
   
   <!-- @Malte: could you explain the "danger" a little more clearly (1 sentence ideally)? Not sure, I see it fully. (Henning)-->
    
   `rnorm(10, mean = 10:1)` generates ten random numbers from different normal distributions. These normal distributions differ in their means. The first has mean 10, the second mean 9, the third mean 8 and so on.

2. __[Q]{.Q}__: Compare the speed of `apply(x, 1, sum)` with `rowSums(x)` for varying sizes of `x`.
    
   __[A]{.solved}__: We compare the two functions for square matrices of increasing size
    
    ```{r, warning=FALSE}
    dimensions <- c(1, 10, 100, 1000, 5000, 10000)
    matrices <- lapply(dimensions,
                       function(x) tcrossprod(rnorm(x), rnorm(x)))
    names(matrices) <- dimensions

        
    bench_res <- matrices %>% 
      purrr::map_dfr(
        ~ bench::mark(
          rowSums(.),
          apply(., 1, sum)
        ) %>% 
          purrr::modify_at("expression", as.character),
        .id = "dimension"
      )

    library(ggplot2)
    bench_res %>% 
      purrr::modify_at("dimension", factor, levels = dimensions) %>% 
      ggplot(aes(dimension, median, color = expression, group = expression)) +
      geom_point() +
      geom_line()
    ```

   We can see, that the difference in performance is negligible for small matrices, but becomes more and more relevant as the size of the data increases. `apply()` is a very versatile tool, but it's not "vectorised for performance" and not as optimized as `rowSums()`.

3. __[Q]{.Q}__: How can you use `crossprod()` to compute a weighted sum? How much faster is it than the naive `sum(x * w)`?
    
   __[A]{.solved}__: We can hand the vectors to `crossprod()`, which converts them to row- and column-vectors and then multiplies these. The result is the dot product, which corresponds to a weighted sum.
    
    ```{r}
    x <- rnorm(10)
    w <- rnorm(10)
    identical(sum(x * w),
              as.numeric(crossprod(x, w)))
    ```
    
   A benchmark of both approaches for different vector lengths indicates that the `crossprod()` variant is almost twice as fast as `sum()`.

    ```{r}
    dimensions <- c(1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 0.5e7, 1e7)
    x_vector <- lapply(dimensions, rnorm)
    w_vector <- lapply(dimensions, rnorm)

    
    bench_res <- purrr::map2_dfr(
      x_vector, w_vector,
      ~ bench::mark(
        sum(.x * .y),
        crossprod(.x, .y)[[1]]
      ) %>% 
        purrr::modify_at("expression", as.character)
    ) %>% 
      dplyr::mutate(dimension = factor(rep(dimensions, each = 2)))

    library(ggplot2)
    bench_res %>% 
      ggplot(aes(dimension, median, color = expression)) +
      geom_point() +
      geom_line(aes(group = expression))
    ```
