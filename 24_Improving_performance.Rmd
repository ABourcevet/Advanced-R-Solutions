```{r, include=FALSE}
source("common.R")
```

# Improving performance

## Checking for existing solutions

1. __[Q]{.Q}__: What are faster alternatives to `lm`? Which are specifically designed to work with larger datasets?

   __[A]{.solved}__: In the [Cran Task View](https://cran.rstudio.com/web/views/) for [High Performance Computing](https://cran.rstudio.com/web/views/HighPerformanceComputing.html) we can find for example the `speedglm` package and its `speedlm()` function. For small datasets, there may be only minor performance gains:

<!-- TODO: replace with penguins! -->

    ```{r}
    stopifnot(all.equal(
      coef(speedglm::speedlm(Sepal.Length ~ Sepal.Width + Species,
                             data = iris)),
      coef(lm(Sepal.Length ~ Sepal.Width + Species,
              data = iris))))

    microbenchmark::microbenchmark(
      speedglm::speedlm(Sepal.Length ~ Sepal.Width + Species,
                        data = iris),
      lm(Sepal.Length ~ Sepal.Width + Species,
         data = iris)
    )
    ```
    
   However on bigger datasets it can make a difference:
    
    ```{r,collapse = TRUE}
    eps <- rnorm(100000)
    x1 <- rnorm(100000, 5, 3)
    x2 <- rep(c("a", "b"), 50000)
    y <- 7 * x1 + (x2 == "a") + eps
    td <- data.frame(y = y, x1 = x1, x2 = x2, eps = eps)
    
    stopifnot(all.equal(
      coef(speedglm::speedlm(y ~ x1 + x2, data = td)),
      coef(lm(y ~ x1 + x2, data = td))))
    
    microbenchmark::microbenchmark(
      speedglm::speedlm(y ~ x1 + x2, data = td),
      lm(y ~ x1 + x2, data = td)
    )
    ```
    
   For further speed improvements, you might consider switching your linear algebra libraries as stated in `?speedglm::speedlm`
    
   > The functions of class 'speedlm' may speed up the fitting of LMs to large data sets. High performances can be obtained especially if R is linked against an optimized BLAS, such as ATLAS.
    
   Note that there are many other opportunities mentioned in the task view, also some that make it possible to handle data which is not in memory. 
    
   When it comes to pure speed a quick search on [r fastest lm](http://stackoverflow.com/questions/25416413/is-there-a-faster-lm-function) provides a stackoverflow thread where someone already solved this problem for us...

2. __[Q]{.Q}__: What package implements a version of `match()` that's faster for repeated lookups? How much faster is it?
    
   __[A]{.solved}__: Again google gives a good recommendation for the search term [r faster match](http://stackoverflow.com/questions/32934933/faster-in-operator):
    
    ```{r}
    set.seed(1)
    table <- 1L:100000L
    x <- sample(table, 10000, replace = TRUE)
    
    stopifnot(all.equal(match(x, table), fastmatch::fmatch(x, table)))
    
    microbenchmark::microbenchmark(
      match(x, table),
      fastmatch::fmatch(x, table)
    )
    ```
    
   On my laptop `fastmatch::fmatch()` is around 25 times as fast as `match()`.

3. __[Q]{.Q}__: List four functions (not just those in base R) that convert a string into a date time object. What are their strengths and weaknesses?
    
   __[A]{.solved}__: The usual base R way is to use the `as.POSIXct()` generic and create a date time object of class POSIXct and type integer.
   
    ```{r}
    date_ct <- as.POSIXct("2020-01-01 12:30:25")
    date_ct
    ```
   Under the hood `as.POSIXct()` employs `as.POSIXlt()` for the character conversion. This creates a date time object of class POSIXlt and type list.
   
    ```{r}
    date_lt <- as.POSIXlt("2020-01-01 12:30:25")
    date_lt
    ```

   The POSIXlt class has the advantage that it carries the individual time components as attributes. This allows to extract the time components via typical list operators.
    
    ```{r}
    attributes(date_lt)
    date_lt$sec
    ```
   
   However, while lists can be a practical data strutcture, basic calculations are often faster and less memory hungry for objects with underlying integer type.
   
    ```{r}
    date_lt2 <- rep(date_lt, 10000)
    date_ct2 <- rep(date_ct, 10000)
    
    bench::mark(
      date_lt2 - date_lt2, 
      date_ct2 - date_ct2
    )
    ```

   Although both date time classes inherit POSIXt as a second class, many functions only dispatch on the first argument and don't account for the possibility of mixed date time object usage. Therefore, better decide for an appropriate date time class, instead of mixing them.
   
    ```{r}
    c(date_lt, date_ct)
    c(date_ct, date_lt)
    ```
    
   `as.POSIXlt()` in turn uses `strptime()` under the hood, which creates a similar date time object.
   
    ```{r}
    date_str <- strptime("2020-01-01 12:30:25",
                         format = "%Y-%m-%d %H:%M:%S")
    identical(date_lt, date_str)
    ```

   While `as.POSIXct()` and `as.POSIXlt()` accept different character inputs by default (e.g. "2001-01-01 12:30", "2001/1/1 12:30"), the format argument when applying `strptime()` needs to be set in any case. If the format is set in any of the converter functions and the input doesn't fit to it, all three functions will return `NA`s.

   On the other hand, specifying the format and using `strptime()` can help to improve the character conversion rate.

    ```{r}
    bench::mark(
      as.POSIXct = as.POSIXct("2020-01-01 12:30:25"),
      as.POSIXct_format = as.POSIXct("2020-01-01 12:30:25",
                                     format = "%Y-%m-%d %H:%M:%S"),
      strptime_fomat = strptime("2020-01-01 12:30:25",
                                format = "%Y-%m-%d %H:%M:%S")
    )
    ```
    
   A fourth way is to use the converter functions from the lubridate package, which are basically wrappers for the POSIXct approach with a more intuitive syntax.

    ```{r}
    library(lubridate)
    ymd_hms("2013-07-24 23:55:26")
    ```

   However, in this case, intuitivity comes with a decrease in performance.

    ```{r}
    bench::mark(
      as.POSIXct = as.POSIXct("2013-07-24 23:55:26", tz = "UTC"),
      ymd_hms = ymd_hms("2013-07-24 23:55:26")
    )
    ```
    
   For further ways to convert characters into date time objects, have a look into the chron, the anytime and the fasttime packages. The chron package has it's own classes and stores times as fractions of days in the underlying double type. However, in contrast to the POSIXct and lt classes, chron doesn't deal with timezones and daylight savings. The anytime package converts (accoring to its description) "Anything to POSIXct or Date". Similarly, the fasttime package and its only function `fastPOSIXct()` are named after what they do best.

4. __[Q]{.Q}__: Which packages provide the ability to compute a rolling mean?
    
   __[A]{.solved}__: Again google [r rolling mean](http://stackoverflow.com/questions/743812/calculating-moving-average-in-r)
provides us with enough information and guides our attention on solutions in the following packages:
    * `zoo`
    ```{r}
    zoo::rollmean(1:10, 2, na.pad = TRUE, align = "left")
    zoo::rollapply(1:10, 2, mean, fill = NA, align = "left")
    ```
    * `TTR`
    ```{r}
    TTR::SMA(1:10, 2)
    ```
    * `RcppRoll`
    ```{r}
    RcppRoll::roll_mean(1:10, n = 2, fill = NA, align = "left")
    ```
    * `caTools`
    ```{r}
    caTools::runmean(1:10, k = 2, endrule = "NA", align = "left")
    ```

   Note that an exhaustive example on how to create a rolling mean function is provided in the [textbook](http://adv-r.had.co.nz/Functionals.html).

5. __[Q]{.Q}__: What are the alternatives to `optim()`?
    
   __[A]{.solved}__: Depending on the use case a lot of different options might be considered. For a general overview we would suggest the corresponding taskview on [Optimization](https://cran.r-project.org/web/views/Optimization.html).

## Doing as little as possible

1. __[Q]{.Q}__: What's the difference between `rowSums()` and `.rowSums()`?
    
   __[A]{.solved}__: `.rowSums()` is defined as
    
    ```{r}
    .rowSums
    ```
    
   this means, that the internal `.rowSums()` function is called via `.Internal()`.
    
   > .Internal performs a call to an internal code which is built in to the R
    interpreter.
    
   The internal `.rowSums()` is a complete different function than the "normal" `rowSums()` function.
    
   Of course (since they have the same name) in this case these functions are heavily related with each other: If we look into the source code of `rowSums()`, we see that it is a wrapper around the internal `.rowSums()`. Just some input checkings, conversions and the special cases (complex numbers) are added:
    
    ```{r}
    rowSums
    ```

2. __[Q]{.Q}__: Make a faster version of `chisq.test()` that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying `chisq.test()` or by coding from the [mathematical definition](http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).
    
   __[A]{.solved}__: Since `chisq.test()` has a relatively long source code, we try a new implementation from scratch:
    
    ```{r}
    chisq.test2 <- function(x, y){
      
      # Input
      if (!is.numeric(x)) {
        stop("x must be numeric")}
      if (!is.numeric(y)) {
        stop("y must be numeric")}
      if (length(x) != length(y)) {
        stop("x and y must have the same length")}
      if (length(x) <= 1) {
        stop("length of x must be greater one")}
      if (any(c(x, y) < 0)) {
        stop("all entries of x and y must be greater or equal zero")}
      if (sum(complete.cases(x, y)) != length(x)) {
        stop("there must be no missing values in x and y")}
      if (any(is.null(c(x, y)))) {
        stop("entries of x and y must not be NULL")}
      
      # Help variables
      m <- rbind(x, y)
      margin1 <- rowSums(m)
      margin2 <- colSums(m)
      n <- sum(m)
      me <- tcrossprod(margin1, margin2) / n
      
      # Output
      x_stat = sum((m - me)^2 / me)
      dof <- (length(margin1) - 1) * (length(margin2) - 1)
      p <- pchisq(x_stat, df = dof, lower.tail = FALSE)
      
      return(list(x_stat = x_stat, df = dof, `p-value` = p))
    }
    ```
    
   We check if our new implementation returns the same results
    
    ```{r}
    a <- 21:25
    b <- c(21, 23, 25, 27, 29)
    m_test <- cbind(a, b)
    
    chisq.test(m_test)
    chisq.test2(a, b)
    ```
    
   Finally we benchmark this implementation against a compiled version of itself and the original `stats::chisq.test()`:
    
    ```{r}
    chisq.test2c <- compiler::cmpfun(chisq.test2)
    
    microbenchmark::microbenchmark(
      chisq.test(m_test),
      chisq.test2(a, b),
      chisq.test2c(a, b)
    )
    ```

3. __[Q]{.Q}__: Can you make a faster version of `table()` for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?
    
   __[A]{.solved}__: When analysing the source code of `table()`, we can omit everything unnecessary and extract the main building blocks. That's where we realize that `table()` is mainly powered by `tabulate()`, which is a very fast counting function. However, we can not simply apply `tabulate()` to our two integer vectors directly. Instead, we need to preprocess them accordingly, to match `tabulate()`'s expected input format. Therefore, the integer pairs in our two vectors need to be mapped to their corresponding index in the final output table (which is ordered columnwise in this implementation). The basic steps to get to this mapping can be adopted from `table()`'s source code again. So, the main challenge is to do these preprocessing steps as performant as possible.

   First, we calculate the dimensions and names of the output table. Therefore, we need to apply `unique()`, `sort()` and `length()` to our integer vectors. Next, we map the elements of each vector according to their position within the vector itself (i.e. the smallest value is mapped to `1L`, the second smallest value to `2L`, ...). One fast way to do these lookups is to employ the combination of `match(x, sort(unique(x)))` or (even better) `fastmatch::fmatch(x, sort(unique(x)))`. Following the logic within `table()`, we combine and shift these looked up values similarly to finally get a mapping from integer pairs in our data to the index of the output table. After applying the lookup, `tabulate()` counts the values for us, and is returning an integer vector which contains counts for each position in our the table. As a last step, we reuse the code from `table()` to assign the correct dimension and class.

    ```{r}
    table2 <- function(a, b){
    
      s_u_a <- sort(unique(a))
      s_u_b <- sort(unique(b))
  
      l_u_a <- length(s_u_a)
      l_u_b <- length(s_u_b)
  
      dims <- c(l_u_a, l_u_b)
      pr <- l_u_a * l_u_b
      dn <- list(a = s_u_a, 
                 b = s_u_b)

      bin <- fastmatch::fmatch(a, s_u_a) +
        l_u_a * fastmatch::fmatch(b, s_u_b) - l_u_a
      y <- tabulate(bin, pr)
      
      y <- array(y, dim = dims, dimnames = dn)
      class(y) <- "table"
      
      y
    }
       
    a <- sample(100, 10000, TRUE)
    b <- sample(100, 10000, TRUE)
    
    bench::mark(table(a, b),
                table2(a, b),
                relative = TRUE)
    ```

## Vectorise

1. __[Q]{.Q}__: The density functions, e.g., `dnorm()`, have a common interface. Which arguments are vectorised over? What does `rnorm(10, mean = 10:1)` do?
    
   __[A]{.solved}__: We can see the interface of these functions via `?dnorm`:
    
    ```{r, eval = FALSE}
    dnorm(x, mean = 0, sd = 1, log = FALSE)
    pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
    qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
    rnorm(n, mean = 0, sd = 1).
    ```
    
   They are vectorised over their numeric arguments, which is always the first argument (`x`, `q`, `p`, `n`), `mean` and `sd`. Note that it's dangerous to supply a vector to `n` in the `rnorm()` function, since the behaviour will change, when `n` has length 1 (like in the second part of this question).
    
   `rnorm(10, mean = 10:1)` generates ten random numbers from different normal distributions. The normal distributions differ in their means. The first has mean 10, the second has mean 9, the third mean 8 etc.

2. __[Q]{.Q}__: Compare the speed of `apply(x, 1, sum)` with `rowSums(x)` for varying sizes of `x`.
    
   __[A]{.solved}__: We compare regarding different sizes for square matrices:
    
    ```{r long calculation, eval=FALSE}
    library(microbenchmark)
    dimensions <- c(1e0, 1e1, 1e2, 1e3, 0.5e4, 1e4)
    matrices <- lapply(dimensions,
                       function(x) tcrossprod(rnorm(x), rnorm(x)))
    
    bench_rs <- lapply(matrices,
                       function(x) fivenum(
                         microbenchmark(rowSums(x),
                                        unit = "ns")$time
                       )
    )
    
    bench_rs <- data.frame(time = unlist(bench_rs), 
                           call = "rowSums", stringsAsFactors = FALSE)
    
    bench_apply <- lapply(matrices,
                          function(x) fivenum(
                            microbenchmark(apply(x, 1, sum),
                                           unit = "ns")$time
                          )
    )
    bench_apply <- data.frame(time = unlist(bench_apply),
                              call = "apply", stringsAsFactors = FALSE)
    
    df <- rbind(bench_rs, bench_apply)
    
    df$dimension <- rep(dimensions, each = 5)
    df$aggr <- rep(c("min", "lq", "median", "uq", "max"),
                   times = length(dimensions))
    df$aggr_size <- rep(c(1, 2, 3, 2, 1), times = length(dimensions))
    df$group <- paste(as.character(df$call), as.character(df$aggr))
    
    library(ggplot2)
    
    ggplot(df, aes(x = dimension, y = time,
                   colour = call, group = group)) +
      geom_point() + 
      geom_line(aes(linetype = factor(aggr_size,
                                      levels = c("3", "2", "1"))),
            show.legend = FALSE) 
    ```
    
   The graph is a good indicator to notice, that `apply()` is not "vectorised for performance".
  
3. __[Q]{.Q}__: How can you use `crossprod()` to compute a weighted sum? How much faster is it than the naive `sum(x * w)`?
    
   __[A]{.solved}__: We can just give the vectors to `crossprod()` which converts them to row- and column-vectors and then multiplies these. The result is the dot product which is also a weighted sum.
    
    ```{r}
    a <- rnorm(10)
    b <- rnorm(10)
    sum(a * b) - crossprod(a, b)[1]
    ```
    
   A benchmark of both alternatives for different vector lengths indicates, that the `crossprod()` variant is about 2.5 times faster than `sum()`:
    
    ```{r, eval=FALSE}
    dimensions <- c(1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 0.5e7, 1e7)
    xvector <- lapply(dimensions, rnorm)
    weights <- lapply(dimensions, rnorm)
    
    bench_sum <- Map(function(x, y) fivenum(
      microbenchmark(sum(x * y))$time
    ),
    xvector, weights)
    bench_sum <- data.frame(time = unlist(bench_sum),
                            call = "sum",
                            stringsAsFactors = FALSE)
    bench_cp <- Map(function(x, y) fivenum(
      microbenchmark(crossprod(x, y)[1])$time
    ),
    xvector, weights)
    bench_cp <- data.frame(time = unlist(bench_cp),
                           call = "crossproduct",
                           stringsAsFactors = FALSE)
    
    df <- rbind(bench_sum, bench_cp)
    
    df$dimension <- rep(dimensions, each = 5)
    df$aggr <- rep(c("min", "lq", "median", "uq", "max"),
                   times = length(dimensions))
    df$aggr_size <- rep(c(1, 2, 3, 2, 1), times = length(dimensions))
    df$group <- paste(as.character(df$call), as.character(df$aggr))
    
    ggplot(df, aes(x = dimension, y = time,
                   colour = call, group = group)) +
      geom_point() + 
      geom_line(aes(linetype = factor(aggr_size,
                                      levels = c("3", "2", "1"))),
                show.legend = FALSE) +
      scale_y_continuous(expand = c(0, 0))
    ```
    