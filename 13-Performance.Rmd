# Performance

## Microbenchmarking

1. __<span style="color:red">Q</span>__: Instead of using `microbenchmark()`, you could use the built-in function
   `system.time()`. But `system.time()` is much less precise, so you'll
   need to repeat each operation many times with a loop, and then divide
   to find the average time of each operation, as in the code below.

    ```{r, eval = FALSE}
    n <- 1:1e6
    system.time(for (i in n) sqrt(x)) / length(n)
    system.time(for (i in n) x ^ 0.5) / length(n)
    ```
    
    How do the estimates from `system.time()` compare to those from
    `microbenchmark()`? Why are they different?

1.  __<span style="color:red">Q</span>__: Here are two other ways to compute the square root of a vector. Which
    do you think will be fastest? Which will be slowest? Use microbenchmarking
    to test your answers.

    ```{r, eval = FALSE}
    x ^ (1 / 2)
    exp(log(x) / 2)
    ```
    
    __<span style="color:green">A</span>__: The second one looks more complex, but you never know...
    
    ```{r}
    x <- runif(100)
    microbenchmark::microbenchmark(
      sqrt(x),
      x ^ 0.5,
      x ^ (1 / 2),
      exp(log(x) / 2)
    )
    ```
    
    ...unless you test it.

1.  __<span style="color:red">Q</span>__: Use microbenchmarking to rank the basic arithmetic operators (`+`, `-`,
    `*`, `/`, and `^`) in terms of their speed. Visualise the results. Compare
    the speed of arithmetic on integers vs. doubles.
    
    __<span style="color:green">A</span>__: Since I am on a Windows system, where these short execution times are hard to measure, I just ran the following code on a linux and pasted the results:
    
    ```{r, eval = FALSE}
    mb_integer <- microbenchmark::microbenchmark(
      1L + 1L, 1L - 1L, 1L * 1L, 1L / 1L, 1L ^ 1L, 
      times = 100000
    )
    
    mb_double <- microbenchmark::microbenchmark(
      1 + 1, 1 - 1, 1 * 1, 1 / 1, 1 ^ 1, 
      times = 100000
    )
    
    mb_integer
    # and got the following output:
    # Unit: nanoseconds
    #     expr min lq     mean median  uq      max neval
    #  1L + 1L  49 72 102.2314     78  93    19892 1e+05
    #  1L - 1L  51 73 102.6916     79  93    19898 1e+05
    #  1L * 1L  50 74 459.0319     81  95 32282526 1e+05
    #    1L/1L  49 71 101.9581     78  92    17111 1e+05
    #    1L^1L  68 86 133.0490     97 111  1333248 1e+05
    
    mb_double
    # Unit: nanoseconds
    #   expr min lq      mean median  uq     max neval
    #  1 + 1  51 70  99.22536     73  80  473456 1e+05
    #  1 - 1  49 69 100.97983     72  80  623871 1e+05
    #  1 * 1  50 70  97.32268     74  83   21533 1e+05
    #    1/1  50 69 111.22211     74  82 1584062 1e+05
    #    1^1  82 98 137.42019    108 129   19486 1e+05
    ```
    
    To visualise and compare the results, we make some short spaghetties:
    
    ```{r}
    mb_median <- data.frame(operator = c("+", "-", "*", "/", "^"),
                            int = c(102.2314, 102.6916, 459.0319, 101.9581, 133.0490),
                            dbl = c(99.22536, 100.97983, 97.32268, 111.22211, 137.42019),
                            stringsAsFactors = FALSE)

    mb_median <- tidyr::gather(mb_median, type, time, int, dbl)
    mb_median <- dplyr::mutate(mb_median, type = factor(type, levels = c("int", "dbl")))

    library(ggplot2)
    ggplot(mb_median, aes(x = type, y = time, group = operator, color = operator)) +
      geom_point(show.legend = FALSE) +
      geom_line(show.legend = FALSE, size = 1.5) +
      geom_label(aes(label = operator), show.legend = FALSE) +
      theme_minimal() +
      ylab("time in nanoseconds") +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_text(size = 14),
            axis.text.x = element_text(size = 14),
            axis.text.y = element_text(size = 10)) +
      scale_y_continuous(breaks=seq(0, max(mb_median$time), 10))
    ```

1.  __<span style="color:red">Q</span>__: You can change the units in which the microbenchmark results are
    expressed with the `unit` parameter. Use `unit = "eps"` to show
    the number of evaluations needed to take 1 second. Repeat the benchmarks
    above with the eps unit. How does this change your intuition for performance?

## Language performance

1.  __<span style="color:red">Q</span>__: `scan()` has the most arguments (21) of any base function. About how
    much time does it take to make 21 promises each time scan is called?
    Given a simple input (e.g., `scan(text = "1 2 3", quiet = T)`) what
    proportion of the total run time is due to creating those promises?

1.  __<span style="color:red">Q</span>__: Read ["Evaluating the Design of the R Language"](http://r.cs.purdue.edu/pub/ecoop12.pdf). What other aspects of the R-language slow it
    down? Construct microbenchmarks to illustrate. 

1.  __<span style="color:red">Q</span>__: How does the performance of S3 method dispatch change with the length
    of the class vector? How does performance of S4 method dispatch change
    with number of superclasses? How about RC?

1.  __<span style="color:red">Q</span>__: What is the cost of multiple inheritance and multiple dispatch on
    S4 method dispatch?

1.  __<span style="color:red">Q</span>__: Why is the cost of name lookup less for functions in the base package?

## Implementations performance

1.  __<span style="color:red">Q</span>__: The performance characteristics of `squish_ife()`, `squish_p()`, and
   `squish_in_place()` vary considerably with the size of `x`. Explore the
   differences. Which sizes lead to the biggest and smallest differences?

1.  __<span style="color:red">Q</span>__: Compare the performance costs of extracting an element from a list, a
    column from a matrix, and a column from a data frame. Do the same for rows.